% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/07_benchmarking_engine.R
\name{run_model_benchmark}
\alias{run_model_benchmark}
\title{Run a benchmark comparison of multiple models, with automated tuning.}
\usage{
run_model_benchmark(
  task,
  model_codes,
  registry,
  n_evals = 20,
  inner_cv_folds = 5,
  outer_resampling
)
}
\arguments{
\item{task}{The \code{mlr3} task object (e.g., TaskClassif, TaskRegr, TaskSurv).}

\item{model_codes}{A character vector of model codes from the registry to compare.}

\item{registry}{The model registry \code{data.frame}.}

\item{n_evals}{The number of tuning evaluations for each tunable model.}

\item{inner_cv_folds}{The number of CV folds for the inner hyperparameter tuning loop.}

\item{outer_resampling}{The \code{mlr3} resampling strategy for the final outer
benchmark comparison (e.g., \code{rsmp("cv", folds = 3)}).}
}
\value{
An \code{mlr3::BenchmarkResult} object containing the performance of all models.
}
\description{
This function orchestrates a complete benchmark workflow. For each specified model,
it automatically wraps it in an AutoTuner if it's marked as tunable, then
runs a benchmark comparison across all models on a given resampling strategy.
}
