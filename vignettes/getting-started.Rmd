---
title: "Getting Started with autoMLR v4.0"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Getting Started with autoMLR v4.0}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
# Load all necessary libraries for the vignette to run self-contained
library(autoMLR)
library(mlr3)
library(mlr3learners) 
library(mlr3pipelines)
library(mlr3viz)
```

# Introduction

Welcome to `autoMLR` v4.0! This guide demonstrates a complete, end-to-end workflow for robust, automated machine learning. We will showcase the full power of the package, from handling imperfect real-world data to benchmarking multiple models and generating a rich, interactive report with model explanations.

Our journey will follow these steps:

1.  **Setup**: Load our package and the central model registry.
2.  **The Task**: Define our machine learning problem using a real-world dataset with missing values.
3.  **Preprocessing**: Build a robust "self-cleaning" pipeline for our models.
4.  **Benchmarking**: Let multiple models, each with automated tuning, compete to find the champion.
5.  **Analysis**: Visualize the benchmark results to declare a winner.
6.  **Final Report**: Automatically generate a comprehensive HTML report for the best **tuned** model.

# Step 1: The Central Console (Setup)

Our journey begins at the central console, where we can see all available models.

```{r registry}
# The registry is the heart of our package, listing all available models
registry <- load_model_registry()
list_available_models(registry)
```

# Step 2: The Challenge (The Task)

We will use the `pima` dataset, a classic medical dataset for predicting diabetes, which famously contains missing values. This allows us to test the real-world robustness of our system.

```{r task}
# Load the Pima Indians Diabetes task
task <- tsk("pima")
task
```

# Step 3: Building "Self-Cleaning" Engines (Preprocessing)

A professional ML system must handle imperfect data. We will construct a preprocessing pipeline that automatically imputes missing values before the data reaches the learner.

```{r preprocessing}
imputer_graph <- po("imputemean") %>>%
  po("imputemode")
```

Now, we will assemble our competing learners, each equipped with this powerful imputation pipeline.

```{r graph-learners}
# Define the models we want to benchmark
models_to_benchmark <- c("DT", "RF", "LR", "XGB")

# Create a list of GraphLearners
learners_to_benchmark <- lapply(models_to_benchmark, function(code) {
  config <- get_model_config(registry, code)
  learner <- create_learner(config)
  full_graph <- imputer_graph %>>% learner
  gl <- GraphLearner$new(full_graph)
  gl$id <- code
  return(gl)
})

names(learners_to_benchmark) <- models_to_benchmark
print(learners_to_benchmark$DT)
```

# Step 4: The Grand Tournament (Benchmarking)

This is the core of `autoMLR`. We provide the engine with our task and the list of our powerful, self-cleaning learners. The engine will then conduct a full tournament using **nested resampling**.

```{r benchmark-run, cache=FALSE}
# FINAL PROTOCOL: cache is set to FALSE to bypass persistent environmental issues.
set.seed(2025) 
benchmark_result <- run_model_benchmark(
  task = task,
  learners = learners_to_benchmark,
  n_evals = 15,       # Inner tuning evaluations
  outer_cv_folds = 3  # Outer competition folds
)
benchmark_result$aggregate()
```

# Step 5: Crowning the Champion (Analysis)

The tournament is over. Let's visualize the results to see which model performed best on average.

```{r benchmark-plot}
plot_benchmark_summary(benchmark_result)
```

# Step 6: The Book of Champions (Final Report)

Finally, we select our champion **tuned** model and generate a complete, interactive HTML report.

```{r final-report}
# FINAL PROTOCOL v4.0: The logic is now hardened to specifically select
# the best-performing model AMONG THOSE THAT WERE ACTUALLY TUNED.
all_results <- benchmark_result$aggregate(msr("classif.auc"))
tuned_results <- all_results[endsWith(learner_id, "_tuned")]

if (nrow(tuned_results) == 0) {
  message("No tuned learners were found in the benchmark results. Skipping report generation.")
} else {
  best_row_idx <- which.max(tuned_results$classif.auc)
  
  if (length(best_row_idx) == 0) {
    stop("Could not determine the best tuned model. All tuned AUC scores might be NA.")
  }
  
  best_row <- tuned_results[best_row_idx, ]
  best_performer_id <- best_row$learner_id
  
  message(paste("Champion tuned model selected for reporting:", best_performer_id))
  
  best_model_code <- sub("_tuned$", "", best_performer_id)
  
  resample_result_for_best <- benchmark_result$resample_result(which(all_results$learner_id == best_performer_id))
  best_model_obj <- resample_result_for_best$learners[[1]]
  
  output_dir <- tempfile()
  set.seed(42)
  test_indices <- sample(task$row_ids, 50)
  test_data_for_shap <- task$data(rows = test_indices)

  if ("AutoTuner" %in% class(best_model_obj)) {
    tuning_instance <- best_model_obj$tuning_instance
    if (is.null(tuning_instance)) {
        # Fallback for older mlr3 versions
        tuning_instance <- best_model_obj$model$tuning_instance
    }

    if (!is.null(tuning_instance)) {
        generate_training_report(
          config = get_model_config(registry, best_model_code),
          tuning_instance = tuning_instance, 
          final_learner = best_model_obj,
          task = task,
          test_data = test_data_for_shap,
          output_dir = output_dir
        )
        message(paste("Final report generated in:", normalizePath(output_dir)))
    } else {
        message("Could not retrieve tuning instance from the AutoTuner object.")
    }
  } else {
      # This case should not be reached due to the filtering logic above
      message("The selected learner was not an AutoTuner, skipping report.")
  }
}

